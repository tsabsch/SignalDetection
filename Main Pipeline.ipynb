{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Pipeline\n",
    "Execute the cells step by step to obtain a prediction and score for your configuration.\n",
    "In order to make the widgets work you might need to execute \n",
    "```\n",
    "jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    "```\n",
    "on your system.\n",
    "\n",
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import data_utils\n",
    "import widget_ui as ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data Sets\n",
    "Load training and test data. Furthermore, create a sample set for quickly calculating transformations such as PCA or performing test runs in the development process.\n",
    "\n",
    "The csv files are too large to load them directly into our computer's memory (e.g. with [pandas](http://pandas.pydata.org/)), therefore we use the [dask](http://dask.pydata.org) package, which is designed to handle big data sets by parallel computing techniques and lazy evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data_utils.load_from('data/all_train.csv', 'data/all_test.csv')\n",
    "sample_data = data_utils.load_sample_data('data/all_sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary that stores the initialized preprocessors for later use\n",
    "preprocessors = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide two methods to remove correlations between the features: [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) and [Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis).\n",
    "\n",
    "__Pearson correlation coefficient:__ Find correlated pairs of features and remove one feature of these two, respectively. All pairs with a Pearson correlation coefficient above the selected threshold are considered correlated.\n",
    "\n",
    "__PCA:__ Apply a principal component analysis, which detects the features contributing the most to the total variance. The number of principal components can be choosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0395ff86efd346e3bb52fa1208971e71"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1297397c495b4cc6a05524b764efc61f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e49beeebea4856a0709774108451b7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ui.preprocessors_ui(preprocessors, sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Classifiers\n",
    "Both classifiers are implemented to iteratively learn on batches of data. In the _offline_ setting, this method enables us to process large amounts of data that do not fit in memory at the same time. In the _online_ setting, it allows for continuous updates of the classifier whenever new data arrives. For efficiency reasons it is useful to wait for enough data to form a batch rather than processing single data instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary that stores the configured classifiers for later use\n",
    "classifiers = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Multilayer Perceptron\n",
    "An MLP is a feedforward neural network. The implementation is based on the [Scikit-learn MLP module](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html). Even though Multilayer Perceptrons have many parameters that can be adapted, we decided to only vary the general structure, because this gives us enough diversity for estimating the performance of this kind of model and keeps the amount of testing manageable.\n",
    "\n",
    "__Num hidden layers:__ This parameter defines the number of hidden layers for the MLP.\n",
    "\n",
    "__Hidden layer sizes:__ For each hidden layer, the size can individually be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07be7b5d632545f5b44396d77f956173"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ui.mlp_ui(classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Naive Bayes\n",
    "The implementation is based on the [Scikit-learn Naive Bayes module](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html). In order to work with the given continuous features, it was necessary to use the assumption of Gaussian distributions with this classifier. The only parameter for this bayesian model is the distribution of class priors, but as we know that for the HEPMASS data set both classes have the same amount of samples, the priors were fixed to [0.5, 0.5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ui.nb_ui(classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "The red button below starts a training run with the selected parameters. As can be seen in the following code snippet, training is performed in an online manner, even though the data set is available offline, for the reasons given in section 4.\n",
    "\n",
    "```python\n",
    "# create empty window\n",
    "window = np.zeros((0, len(data.train_data.columns)))\n",
    "\n",
    "# process instances in a continuous stream\n",
    "iterator = data.train_data.iterrows()\n",
    "for idx, row in enumerate(iterator):\n",
    "    # add to window\n",
    "    window = np.append(window, [row[1]], axis=0)\n",
    "    \n",
    "    if len(window) == window_size:\n",
    "        train_on_window(preprocessors, classifier, pd.DataFrame(window, columns=data.train_data.columns))\n",
    "        \n",
    "        # reset window\n",
    "        window = np.zeros((0, len(data.train_data.columns)))\n",
    "```\n",
    "\n",
    "The user can specify each of the following parameters:\n",
    "\n",
    "__Use fast sample data:__ Instead of the whole training set, use a small subsample for quick test runs. This option should only be used in the development process, because the sample set is not very representative.\n",
    "\n",
    "__Window size:__ This parameter defines the size of a window/batch for the training process. Batches are collected until they reach this size and then they are given to the classifier for training.\n",
    "\n",
    "__Classifier:__ Select one of the previously configured classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ui.training_ui(data, sample_data, preprocessors, classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prediction\n",
    "On click of the blue button, the previously trained model is used to predict labels for the test data. As in the training step, an online strategy is used and the same parameters can be specified. \n",
    "\n",
    "__Save prediction to file:__ The resulting confusion matrix can be saved to a file in the local `results/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ui.prediction_ui(data, sample_data, preprocessors, classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 7. Results\n",
    "A comparison of different parameter settings in terms of test accuracy can be found in the following table:\n",
    "\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;border-color:#aabcfe;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#aabcfe;color:#669;background-color:#e8edff;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#aabcfe;color:#039;background-color:#b9c9fe;}\n",
    ".tg .tg-1d7g{font-weight:bold;font-size:16px;vertical-align:top}\n",
    ".tg .tg-qv16{font-weight:bold;font-size:16px;text-align:center;vertical-align:top}\n",
    ".tg .tg-qgsu{font-size:15px;vertical-align:top}\n",
    ".tg .tg-yw4l{vertical-align:top}\n",
    ".tg .tg-6k2t{background-color:#D2E4FC;vertical-align:top}\n",
    ".tg .tg-2nw2{background-color:#D2E4FC;font-size:15px;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-yw4l\"></th>\n",
    "    <th class=\"tg-yw4l\"></th>\n",
    "    <th class=\"tg-1d7g\">Naive Bayes<br></th>\n",
    "    <th class=\"tg-qv16\" colspan=\"4\">Multilayer Perceptron (with network sizes)<br></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\"></td>\n",
    "    <td class=\"tg-6k2t\"></td>\n",
    "    <td class=\"tg-qgsu\"></td>\n",
    "    <td class=\"tg-2nw2\">20, 20<br></td>\n",
    "    <td class=\"tg-qgsu\">40, 20<br></td>\n",
    "    <td class=\"tg-2nw2\">20, 20, 20, 20<br></td>\n",
    "    <td class=\"tg-qgsu\">20^10<br></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-1d7g\">None<br></td>\n",
    "    <td class=\"tg-2nw2\"></td>\n",
    "    <td class=\"tg-yw4l\">0.7956</td>\n",
    "    <td class=\"tg-6k2t\">0.8447</td>\n",
    "    <td class=\"tg-yw4l\">0.8427</td>\n",
    "    <td class=\"tg-6k2t\">0.8390</td>\n",
    "    <td class=\"tg-yw4l\">0.846</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-1d7g\" rowspan=\"3\">PCA</td>\n",
    "    <td class=\"tg-2nw2\">3</td>\n",
    "    <td class=\"tg-yw4l\">0.8002</td>\n",
    "    <td class=\"tg-6k2t\">0.7979</td>\n",
    "    <td class=\"tg-yw4l\">0.7955</td>\n",
    "    <td class=\"tg-6k2t\">0.8032</td>\n",
    "    <td class=\"tg-yw4l\">0.8003</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-2nw2\">6</td>\n",
    "    <td class=\"tg-yw4l\">0.7986</td>\n",
    "    <td class=\"tg-6k2t\">0.8064</td>\n",
    "    <td class=\"tg-yw4l\">0.8025</td>\n",
    "    <td class=\"tg-6k2t\">0.8156</td>\n",
    "    <td class=\"tg-yw4l\">0.812</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-2nw2\">15</td>\n",
    "    <td class=\"tg-yw4l\">0.8047</td>\n",
    "    <td class=\"tg-6k2t\">0.8319</td>\n",
    "    <td class=\"tg-yw4l\">0.8305</td>\n",
    "    <td class=\"tg-6k2t\">0.8311</td>\n",
    "    <td class=\"tg-yw4l\">0.8257</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-1d7g\" rowspan=\"3\">Pearson</td>\n",
    "    <td class=\"tg-2nw2\">0.6 (4 features removed)<br></td>\n",
    "    <td class=\"tg-yw4l\">0.7622</td>\n",
    "    <td class=\"tg-6k2t\">0.8044</td>\n",
    "    <td class=\"tg-yw4l\">0.825</td>\n",
    "    <td class=\"tg-6k2t\">0.7984</td>\n",
    "    <td class=\"tg-yw4l\">0.8073</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-2nw2\">0.4 (9 features removed)<br></td>\n",
    "    <td class=\"tg-yw4l\">0.7514</td>\n",
    "    <td class=\"tg-6k2t\">0.7971</td>\n",
    "    <td class=\"tg-yw4l\">0.7806</td>\n",
    "    <td class=\"tg-6k2t\">0.7976</td>\n",
    "    <td class=\"tg-yw4l\">0.8082</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-2nw2\">0.2 (16 features removed)<br></td>\n",
    "    <td class=\"tg-yw4l\">0.5273</td>\n",
    "    <td class=\"tg-6k2t\">0.5456</td>\n",
    "    <td class=\"tg-yw4l\">0.5255</td>\n",
    "    <td class=\"tg-6k2t\">0.5603</td>\n",
    "    <td class=\"tg-yw4l\">0.5693</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "The Multilayer Perceptron in general performed better than the Naive Bayes classifier. \n",
    "\n",
    "Notable is the MLP's ability to work well on raw data. Here, the best results could be achieved without preprocessing. The more features we removed, the more the accuracy decreased on the test set. Since the training set contains several million instances it is possible to successfully train even complex network architectures regardless of many input data dimensions.\n",
    "\n",
    "The Naive Bayes classifier expects normally distributed features that are independent of each other given the class. Our preliminary analysis of the data set showed that several features could be approximated by a Gaussian distribution, while others could not. The latter are most likely the main reason for the poor classification performance. Applying a PCA improved the results compared to using the raw data since it removes correlations in the data. Removing features based on the Pearson Correlation however worsened the accuracy in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Outlook\n",
    "The MLP results could be further improved by a detailed analysis of the training process. Especially an appropriate value for the learning rate should be determined since training is usually very sensitive to this parameter. Monitoring the test accuracy over time could give hints whether the model needs more iterations of training or already started to overfit the training data set. To prevent overfitting and achieve a better generalization techniques like dropout could be included.\n",
    "\n",
    "For the Naive Bayes classifier a more thoughtful selection of the features could help to improve the performance. Features that are not normally distributed (e.g. mass) should be removed before the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
